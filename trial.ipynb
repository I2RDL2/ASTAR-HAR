{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-8IQRTUtzgD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "511bf876-a8ad-4d88-c359-d82531c9a9f9"
      },
      "source": [
        "!git clone https://github.com/I2RDL2/ASTAR-HAR.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'ASTAR-HAR' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjckRfJLt_7c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "0bb72443-7dbe-47d2-9b0a-545524c15c51"
      },
      "source": [
        "import numpy as np\n",
        "data=np.load(\"ASTAR-HAR/HAR_DATA.npz\")\n",
        "lst = data.files\n",
        "for item in lst:\n",
        "    print(item)\n",
        "    # print(data[item])"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "trainx\n",
            "trainy\n",
            "testy\n",
            "testx\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAQ15LpHwRzU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c8e7ace1-d85e-4039-a625-4c440b586738"
      },
      "source": [
        "%%writefile /content/ASTAR-HAR/experiments/har/har.py\n",
        "\n",
        "\n",
        "# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n",
        "#\n",
        "# This work is licensed under the Creative Commons Attribution-NonCommercial\n",
        "# 4.0 International License. To view a copy of this license, visit\n",
        "# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n",
        "# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n",
        "\n",
        "\"\"\"CIFAR-10 final evaluation\"\"\"\n",
        "\n",
        "#changed imports to suit colab\n",
        "import logging\n",
        "import sys,os\n",
        "sys.path.append('/content/ASTAR-HAR/experiments/har/')\n",
        "\n",
        "import run_context\n",
        "# from experiments.run_context import RunContext\n",
        "# from experiments.run_context import Training_log_plot\n",
        "##\n",
        "\n",
        "\n",
        "# from experiments.run_context import Training_log_plot\n",
        "import tensorflow as tf\n",
        "sys.path.append('/content/ASTAR-HAR/')\n",
        "import datasets\n",
        "from mean_teacher.arguments import args\n",
        "from mean_teacher.mean_teacher_base import mean_teacher_base as mean_teacher\n",
        "from mean_teacher import minibatching\n",
        "\n",
        "\n",
        "LOG = logging.getLogger('main')\n",
        "\n",
        "data_loader = getattr(datasets, args.dataset)\n",
        "\n",
        "def parameters(): \n",
        "    for n_labeled in args.n_labeled:\n",
        "        for data_seed in range(args.init_run, args.init_run + args.n_runs):\n",
        "            yield {\n",
        "                'n_labeled': n_labeled,\n",
        "                'data_seed': data_seed\n",
        "            }\n",
        "\n",
        "def run(n_labeled, data_seed):\n",
        "\n",
        "    data = data_loader(n_labeled=n_labeled,\n",
        "                    data_seed=data_seed,\n",
        "                    test_phase=True,\n",
        "                    dataset_detail = args.dataset_detail)\n",
        "\n",
        "    print('{} is loaded with {} of training samples'.format(args.dataset,data['num_train']))\n",
        "\n",
        "    if n_labeled == 'all':\n",
        "        args.n_labeled_per_batch =  args.minibatch_size\n",
        "        args.max_consistency_cost = args.minibatch_size\n",
        "    else:\n",
        "        if args.max_consistency_cost != 0 :\n",
        "            args.max_consistency_cost = args.minibatch_size* int(n_labeled) / data['num_train']\n",
        "\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "    runner_name = os.path.basename(__file__).split(\".\")[0]\n",
        "    runner_name = args.save\n",
        "    file_name = '{}_{}'.format(runner_name,n_labeled)\n",
        "    log_plot = Training_log_plot(file_name,data_seed)\n",
        "    model = mean_teacher(RunContext(file_name, data_seed), args,log_plot)\n",
        "\n",
        "    training_batches = minibatching.training_batches(data.training,\n",
        "                                                     args.minibatch_size,\n",
        "                                                     args.n_labeled_per_batch)\n",
        "    evaluation_batches_fn = minibatching.evaluation_epoch_generator(data.evaluation,\n",
        "                                                                    args.minibatch_size)\n",
        "    test_batches_fn = minibatching.evaluation_epoch_generator(data.evaluation,\n",
        "                                                                    batch_size = 260)\n",
        "\n",
        "\n",
        "    # import pdb; pdb.set_trace()\n",
        "    if args.test_only:\n",
        "\n",
        "        print('loading folers')\n",
        "        root_path = \"./results/\"\n",
        "        folders = os.listdir(root_path)\n",
        "        assert args.ckp != '.','No ckp info was input'\n",
        "        for i in range (len(folders)):\n",
        "            folders[i] = os.path.join(root_path,folders[i])\n",
        "            #print(folders[i])\n",
        "        for folder in folders:\n",
        "            if args.ckp in folder:\n",
        "                print(folder)\n",
        "\n",
        "                matrix = []\n",
        "                for random_seed in os.listdir(folder):\n",
        "                    ckp_path = os.path.join(folder,random_seed,'transient')\n",
        "                    ckp = tf.train.latest_checkpoint(ckp_path)\n",
        "                    print('restore checkpoint from {}'.format(ckp))\n",
        "                    model.restore(ckp)\n",
        "\n",
        "                    confuse_matrix = model.confusion_matrix(test_batches_fn)\n",
        "                    print(acc_from_confuse(confuse_matrix))\n",
        "                    acc_matrix = acc_from_confuse(confuse_matrix)\n",
        "                    matrix.append(acc_matrix)\n",
        "\n",
        "                save_confuse_matrix(matrix,ckp_path)\n",
        "\n",
        "    else:\n",
        "        model.train(training_batches, evaluation_batches_fn)\n",
        "\n",
        "import numpy as np\n",
        "def acc_from_confuse(matrix):\n",
        "    num_sample_cls = np.sum(matrix,axis=1)\n",
        "    correct = np.diag(matrix)\n",
        "    accuracy = correct/num_sample_cls \n",
        "    return accuracy\n",
        "\n",
        "def save_confuse_matrix(matrix,ckp_path):\n",
        "    matrix = np.asarray(matrix)\n",
        "    average = np.mean(matrix,axis=0)\n",
        "    # var = np.std(average)\n",
        "    csv_path = './results/csv/confuse_matrix/'+ckp_path.split('/')[2]+'_avg_conf.csv'\n",
        "    # average.to_csv(csv_path)\n",
        "    np.savetxt(csv_path,average)\n",
        "    print('File saved as {}'.format(csv_path))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu)\n",
        "    for run_params in parameters():\n",
        "        run(**run_params)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /content/ASTAR-HAR/experiments/har/har.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUFROayrxZra",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==1.2.1 numpy scipy pandas"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF9q7u66xfKV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "f173c3a8-b830-44bb-bb61-f9f621d6f7c5"
      },
      "source": [
        "!python3 /content/ASTAR-HAR/experiments/har/har.py --template X"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:459: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:460: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:461: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:462: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:465: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/ASTAR-HAR/experiments/har/har.py\", line 126, in <module>\n",
            "    run(**run_params)\n",
            "  File \"/content/ASTAR-HAR/experiments/har/har.py\", line 48, in run\n",
            "    dataset_detail = args.dataset_detail)\n",
            "TypeError: __init__() got an unexpected keyword argument 'dataset_detail'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKXsN6kJBOz0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}