{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/I2RDL2/ASTAR-HAR/blob/arunraja_trials/trial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7-8IQRTUtzgD",
        "colab_type": "code",
        "outputId": "a327c0c5-64b2-47ce-9b66-ddd22700e644",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 137
        }
      },
      "source": [
        "!git clone https://github.com/I2RDL2/ASTAR-HAR.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'ASTAR-HAR'...\n",
            "remote: Enumerating objects: 27, done.\u001b[K\n",
            "remote: Counting objects:   3% (1/27)\u001b[K\rremote: Counting objects:   7% (2/27)\u001b[K\rremote: Counting objects:  11% (3/27)\u001b[K\rremote: Counting objects:  14% (4/27)\u001b[K\rremote: Counting objects:  18% (5/27)\u001b[K\rremote: Counting objects:  22% (6/27)\u001b[K\rremote: Counting objects:  25% (7/27)\u001b[K\rremote: Counting objects:  29% (8/27)\u001b[K\rremote: Counting objects:  33% (9/27)\u001b[K\rremote: Counting objects:  37% (10/27)\u001b[K\rremote: Counting objects:  40% (11/27)\u001b[K\rremote: Counting objects:  44% (12/27)\u001b[K\rremote: Counting objects:  48% (13/27)\u001b[K\rremote: Counting objects:  51% (14/27)\u001b[K\rremote: Counting objects:  55% (15/27)\u001b[K\rremote: Counting objects:  59% (16/27)\u001b[K\rremote: Counting objects:  62% (17/27)\u001b[K\rremote: Counting objects:  66% (18/27)\u001b[K\rremote: Counting objects:  70% (19/27)\u001b[K\rremote: Counting objects:  74% (20/27)\u001b[K\rremote: Counting objects:  77% (21/27)\u001b[K\rremote: Counting objects:  81% (22/27)\u001b[K\rremote: Counting objects:  85% (23/27)\u001b[K\rremote: Counting objects:  88% (24/27)\u001b[K\rremote: Counting objects:  92% (25/27)\u001b[K\rremote: Counting objects:  96% (26/27)\u001b[K\rremote: Counting objects: 100% (27/27)\u001b[K\rremote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 106 (delta 11), reused 4 (delta 1), pack-reused 79\u001b[K\n",
            "Receiving objects: 100% (106/106), 49.96 MiB | 24.28 MiB/s, done.\n",
            "Resolving deltas: 100% (17/17), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjckRfJLt_7c",
        "colab_type": "code",
        "outputId": "bdeace56-fbd6-485e-91f1-0a6cdd8e63a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import numpy as np\n",
        "data=np.load(\"ASTAR-HAR/HAR_DATA.npz\")\n",
        "print(data.files)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['trainx', 'trainy', 'testy', 'testx']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAQ15LpHwRzU",
        "colab_type": "code",
        "outputId": "1291c334-4a9d-45ad-c147-6895907f523f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%%writefile /content/ASTAR-HAR/experiments/har/har.py\n",
        "\n",
        "\n",
        "# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n",
        "#\n",
        "# This work is licensed under the Creative Commons Attribution-NonCommercial\n",
        "# 4.0 International License. To view a copy of this license, visit\n",
        "# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n",
        "# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n",
        "\n",
        "\"\"\"CIFAR-10 final evaluation\"\"\"\n",
        "\n",
        "#changed imports to suit colab\n",
        "import logging\n",
        "import sys,os\n",
        "sys.path.append('/content/ASTAR-HAR/experiments/')\n",
        "\n",
        "import run_context\n",
        "# from experiments.run_context import RunContext\n",
        "# from experiments.run_context import Training_log_plot\n",
        "##\n",
        "\n",
        "\n",
        "# from experiments.run_context import Training_log_plot\n",
        "import tensorflow as tf\n",
        "sys.path.append('/content/ASTAR-HAR/')\n",
        "import datasets\n",
        "from mean_teacher.arguments import args\n",
        "from mean_teacher.mean_teacher_base import mean_teacher_base as mean_teacher\n",
        "from mean_teacher import minibatching\n",
        "\n",
        "\n",
        "LOG = logging.getLogger('main')\n",
        "\n",
        "data_loader = getattr(datasets, args.dataset)\n",
        "\n",
        "def parameters(): \n",
        "    for n_labeled in args.n_labeled:\n",
        "        for data_seed in range(args.init_run, args.init_run + args.n_runs):\n",
        "            yield {\n",
        "                'n_labeled': n_labeled,\n",
        "                'data_seed': data_seed\n",
        "            }\n",
        "\n",
        "def run(n_labeled, data_seed):\n",
        "\n",
        "    data = data_loader(n_labeled=n_labeled,\n",
        "                    data_seed=data_seed,\n",
        "                    test_phase=True)\n",
        "\n",
        "    print('{} is loaded with {} of training samples'.format(args.dataset,data['num_train']))\n",
        "\n",
        "    if n_labeled == 'all':\n",
        "        args.n_labeled_per_batch =  args.minibatch_size\n",
        "        args.max_consistency_cost = args.minibatch_size\n",
        "    else:\n",
        "        if args.max_consistency_cost != 0 :\n",
        "            args.max_consistency_cost = args.minibatch_size* int(n_labeled) / data['num_train']\n",
        "\n",
        "\n",
        "    tf.reset_default_graph()\n",
        "    runner_name = os.path.basename(__file__).split(\".\")[0]\n",
        "    runner_name = args.save\n",
        "    file_name = '{}_{}'.format(runner_name,n_labeled)\n",
        "    log_plot = run_context.Training_log_plot(file_name,data_seed)\n",
        "    model = mean_teacher(run_context.RunContext(file_name, data_seed), args,log_plot)\n",
        "\n",
        "    training_batches = minibatching.training_batches(data.training,\n",
        "                                                     args.minibatch_size,\n",
        "                                                     args.n_labeled_per_batch)\n",
        "    evaluation_batches_fn = minibatching.evaluation_epoch_generator(data.evaluation,\n",
        "                                                                    args.minibatch_size)\n",
        "    test_batches_fn = minibatching.evaluation_epoch_generator(data.evaluation,\n",
        "                                                                    batch_size = 260)\n",
        "\n",
        "\n",
        "    # import pdb; pdb.set_trace()\n",
        "    if args.test_only:\n",
        "\n",
        "        print('loading folers')\n",
        "        root_path = \"./results/\"\n",
        "        folders = os.listdir(root_path)\n",
        "        assert args.ckp != '.','No ckp info was input'\n",
        "        for i in range (len(folders)):\n",
        "            folders[i] = os.path.join(root_path,folders[i])\n",
        "            #print(folders[i])\n",
        "        for folder in folders:\n",
        "            if args.ckp in folder:\n",
        "                print(folder)\n",
        "\n",
        "                matrix = []\n",
        "                for random_seed in os.listdir(folder):\n",
        "                    ckp_path = os.path.join(folder,random_seed,'transient')\n",
        "                    ckp = tf.train.latest_checkpoint(ckp_path)\n",
        "                    print('restore checkpoint from {}'.format(ckp))\n",
        "                    model.restore(ckp)\n",
        "\n",
        "                    confuse_matrix = model.confusion_matrix(test_batches_fn)\n",
        "                    print(acc_from_confuse(confuse_matrix))\n",
        "                    acc_matrix = acc_from_confuse(confuse_matrix)\n",
        "                    matrix.append(acc_matrix)\n",
        "\n",
        "                save_confuse_matrix(matrix,ckp_path)\n",
        "\n",
        "    else:\n",
        "        model.train(training_batches, evaluation_batches_fn)\n",
        "\n",
        "import numpy as np\n",
        "def acc_from_confuse(matrix):\n",
        "    num_sample_cls = np.sum(matrix,axis=1)\n",
        "    correct = np.diag(matrix)\n",
        "    accuracy = correct/num_sample_cls \n",
        "    return accuracy\n",
        "\n",
        "def save_confuse_matrix(matrix,ckp_path):\n",
        "    matrix = np.asarray(matrix)\n",
        "    average = np.mean(matrix,axis=0)\n",
        "    # var = np.std(average)\n",
        "    csv_path = './results/csv/confuse_matrix/'+ckp_path.split('/')[2]+'_avg_conf.csv'\n",
        "    # average.to_csv(csv_path)\n",
        "    np.savetxt(csv_path,average)\n",
        "    print('File saved as {}'.format(csv_path))\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(args.gpu)\n",
        "    for run_params in parameters():\n",
        "        run(**run_params)\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /content/ASTAR-HAR/experiments/har/har.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oUFROayrxZra",
        "colab_type": "code",
        "outputId": "17eb367a-e83e-484f-bae5-f32479cc5cf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        }
      },
      "source": [
        "!pip uninstall tensorflow\n",
        "!pip install tensorflow==1.2.1 numpy scipy pandas"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-1.2.1:\n",
            "  Would remove:\n",
            "    /usr/local/bin/saved_model_cli\n",
            "    /usr/local/bin/tensorboard\n",
            "    /usr/local/lib/python3.6/dist-packages/external/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow-1.2.1.dist-info/*\n",
            "    /usr/local/lib/python3.6/dist-packages/tensorflow/*\n",
            "Proceed (y/n)? y\n",
            "  Successfully uninstalled tensorflow-1.2.1\n",
            "Collecting tensorflow==1.2.1\n",
            "  Using cached https://files.pythonhosted.org/packages/7d/d0/96269b9ecfcc55cb38779831595e0521c34ef4ecdeba08b1ba4194cc4813/tensorflow-1.2.1-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (1.0.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2.1) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2.1) (3.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2.1) (1.0.1)\n",
            "Requirement already satisfied: backports.weakref==1.0rc1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2.1) (1.0rc1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2.1) (3.2.1)\n",
            "Requirement already satisfied: html5lib==0.9999999 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2.1) (0.9999999)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2.1) (0.34.2)\n",
            "Requirement already satisfied: bleach==1.5.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.2.1) (1.5.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas) (2.8.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.2.0->tensorflow==1.2.1) (46.1.3)\n",
            "Installing collected packages: tensorflow\n",
            "Successfully installed tensorflow-1.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pF9q7u66xfKV",
        "colab_type": "code",
        "outputId": "3a15d9f4-ef72-4329-e347-50f885fa958f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        }
      },
      "source": [
        "!python3 /content/ASTAR-HAR/experiments/har/har.py  --dataset 'HAR'"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:459: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:460: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:461: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:462: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:465: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/ASTAR-HAR/experiments/har/har.py\", line 126, in <module>\n",
            "    run(**run_params)\n",
            "  File \"/content/ASTAR-HAR/experiments/har/har.py\", line 48, in run\n",
            "    test_phase=True)\n",
            "  File \"/content/ASTAR-HAR/datasets/har.py\", line 35, in __init__\n",
            "    self._load()\n",
            "  File \"/content/ASTAR-HAR/datasets/har.py\", line 46, in _load\n",
            "    file_data = np.load(self.DATA_PATH)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/numpy/lib/npyio.py\", line 428, in load\n",
            "    fid = open(os_fspath(file), \"rb\")\n",
            "FileNotFoundError: [Errno 2] No such file or directory: 'data/har/HAR_DATA_lok1.npz'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t1IsUByHUIwt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%pycat  /content/ASTAR-HAR/datasets/har.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vELO19mSKu1a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a8cb696a-09e5-4773-a86e-1bb015c16aec"
      },
      "source": [
        "\n",
        "%%writefile /content/ASTAR-HAR/datasets/har.py\n",
        "\n",
        "### two agumentation of audio data is needed\n",
        "### 1. horizontal translation\n",
        "### 2. add bg noise with different level\n",
        "###     This noise can be generated in advance as single category and used \n",
        "###     as augmentation later\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from .utils import random_balanced_partitions, random_partitions, random_ratio_partitions\n",
        "\n",
        "class HAR:\n",
        "  # data', 'har', \n",
        "    DATA_PATH = os.path.join('ASTAR-HAR','HAR_DATA.npz')\n",
        "    VALIDATION_SET_RATIO = 0.1  # 10% of the training set will be used as validation set\n",
        "    VALIDATION_SET_SIZE = 5000\n",
        "    UNLABELED = -1  # We will be using -1 for unlabeled\n",
        "    input_dim = (9,64,1)\n",
        "    label_dim = ()\n",
        "    num_train = 0\n",
        "\n",
        "    # n_labeled for number of labled data, all for using all labels\n",
        "    # more_data stands for additional data without labels used for training\n",
        "    # besides the original 4500 training data\n",
        "    def __init__(self, data_seed=0, n_labeled='all', test_phase=False, \n",
        "        f_sup = False,dataset_detail=None):\n",
        "        \n",
        "        # dataset_detail[0] for path of numpy file\n",
        "        # dataset_detail[1] for method of sampling labeled\n",
        "        self.ratio_sampling = None\n",
        "        if dataset_detail!=None:\n",
        "            self.DATA_PATH = dataset_detail[0]\n",
        "            self.ratio_sampling = dataset_detail[1]\n",
        "                \n",
        "        random = np.random.RandomState(seed=data_seed)\n",
        "        self._load()\n",
        "        if test_phase:\n",
        "            self.evaluation, self.training = self._test_and_training()\n",
        "        else:\n",
        "            self.evaluation, self.training = self._validation_and_training(random)\n",
        "\n",
        "        if n_labeled != 'all':\n",
        "            n_labeled = int(n_labeled)\n",
        "            self.training = self._unlabel(self.training, n_labeled, random)\n",
        "\n",
        "    def _load(self):\n",
        "        file_data = np.load(self.DATA_PATH)\n",
        "\n",
        "#changes made to file names in file_data: train_x->trainx\n",
        "#['trainx', 'trainy', 'testy', 'testx']\n",
        "        NUM_TRAIN = len(file_data['trainx'])\n",
        "        self.num_train=NUM_TRAIN\n",
        "        NUM_TEST = len(file_data['testx'])\n",
        "        self._train_data = self._data_array(NUM_TRAIN, file_data['trainx'], file_data['trainy'])\n",
        "        self._test_data = self._data_array(NUM_TEST, file_data['testx'], file_data['testy'])\n",
        "\n",
        "    def _data_array(self, expected_n, x_data, y_data):\n",
        "        array = np.zeros(expected_n, dtype=[\n",
        "            ('x', np.float32, self.input_dim[:-1]),\n",
        "            ('y', np.int32, self.label_dim)  \n",
        "        ])\n",
        "        array['x'] = x_data\n",
        "        array['y'] = y_data\n",
        "        return array\n",
        "\n",
        "    def _validation_and_training(self, random):\n",
        "        return random_partitions(self._train_data, self.VALIDATION_SET_SIZE, random)\n",
        "\n",
        "    def _test_and_training(self):\n",
        "        return self._test_data, self._train_data\n",
        "\n",
        "    def _unlabel(self, data, n_labeled, random):\n",
        "\n",
        "        if self.ratio_sampling:\n",
        "            labeled, unlabeled = random_ratio_partitions(\n",
        "                data, n_labeled, labels=data['y'], random=random)\n",
        "        else:\n",
        "            labeled, unlabeled = random_balanced_partitions(\n",
        "                data, n_labeled, labels=data['y'], random=random)            \n",
        "\n",
        "        unlabeled['y'] = self.UNLABELED\n",
        "        new_labeled = labeled.copy()\n",
        "        new_labeled['y']=-1\n",
        "        self.num_train +=len(new_labeled)\n",
        "        return np.concatenate([labeled, unlabeled,new_labeled])\n",
        "\n",
        "    def __getitem__(self, key):\n",
        "        return getattr(self,key)\n"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /content/ASTAR-HAR/datasets/har.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zx2pGpcLsFc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 880
        },
        "outputId": "2111bc65-39d8-4fc4-ff21-f46c80f37aa8"
      },
      "source": [
        "!python3 /content/ASTAR-HAR/experiments/har/har.py  --dataset 'HAR'"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:459: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:460: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:461: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:462: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:465: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "not divisible: 1000/24\n",
            "HAR is loaded with 18367 of training samples\n",
            "tower is initliazed!\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/ASTAR-HAR/experiments/har/har.py\", line 126, in <module>\n",
            "    run(**run_params)\n",
            "  File \"/content/ASTAR-HAR/experiments/har/har.py\", line 65, in run\n",
            "    model = mean_teacher(run_context.RunContext(file_name, data_seed), args,log_plot)\n",
            "  File \"/content/ASTAR-HAR/mean_teacher/mean_teacher_base.py\", line 77, in __init__\n",
            "    self.forward()\n",
            "  File \"/content/ASTAR-HAR/mean_teacher/mean_teacher_base.py\", line 120, in forward\n",
            "    translate=self.hyper['translate'])\n",
            "  File \"/content/ASTAR-HAR/mean_teacher/mean_teacher_base.py\", line 314, in inference\n",
            "    _ = self.cnn(**tower_args, dropout_probability=student_dropout_probability, is_initialization=True)\n",
            "  File \"/content/ASTAR-HAR/mean_teacher/model.py\", line 58, in tower\n",
            "    lambda: net)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 289, in new_func\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1814, in cond\n",
            "    orig_res_t, res_t = context_t.BuildCondBranch(true_fn)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1689, in BuildCondBranch\n",
            "    original_result = fn()\n",
            "  File \"/content/ASTAR-HAR/mean_teacher/model.py\", line 57, in <lambda>\n",
            "    lambda: nn.random_translate(net, scale=2, name='random_translate'),\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 181, in func_with_args\n",
            "    return func(*args, **current_args)\n",
            "  File \"/content/ASTAR-HAR/mean_teacher/nn.py\", line 234, in random_translate\n",
            "    return tf.cond(is_training, do_translate, lambda: inputs, name=scope)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py\", line 289, in new_func\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1814, in cond\n",
            "    orig_res_t, res_t = context_t.BuildCondBranch(true_fn)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/control_flow_ops.py\", line 1689, in BuildCondBranch\n",
            "    original_result = fn()\n",
            "  File \"/content/ASTAR-HAR/mean_teacher/nn.py\", line 229, in do_translate\n",
            "    offset_heights = random_offsets(batch_size, -scale[0], scale[0], 'offset_heights')\n",
            "TypeError: 'int' object is not subscriptable\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSe5WW27LunK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%pycat /content/ASTAR-HAR/mean_teacher/nn.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrcDHo2BZC3P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d25ecab8-bfe0-47b8-a00c-8dcf80864ac5"
      },
      "source": [
        "\n",
        "%%writefile /content/ASTAR-HAR/mean_teacher/nn.py\n",
        "\n",
        "# Copyright (c) 2018, Curious AI Ltd. All rights reserved.\n",
        "#\n",
        "# This work is licensed under the Creative Commons Attribution-NonCommercial\n",
        "# 4.0 International License. To view a copy of this license, visit\n",
        "# http://creativecommons.org/licenses/by-nc/4.0/ or send a letter to\n",
        "# Creative Commons, PO Box 1866, Mountain View, CA 94042, USA.\n",
        "\n",
        "\"Functions for building neural networks with Tensorflow\"\n",
        "\n",
        "import logging\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib import slim\n",
        "from tensorflow.contrib import opt\n",
        "from .framework import assert_shape\n",
        "\n",
        "\n",
        "LOG = logging.getLogger('main')\n",
        "\n",
        "\n",
        "@slim.add_arg_scope\n",
        "def step_noise(inputs, step_size, is_training,  name=None):\n",
        "    with tf.name_scope(name, 'step_noise', [inputs, step_size, is_training]) as scope:\n",
        "\n",
        "        def do_add():\n",
        "            # add noise\n",
        "            index = tf.random_uniform([tf.shape(inputs)[0]],minval=0,maxval=10,dtype=tf.int32)\n",
        "            #onehot\n",
        "            index = tf.one_hot(index,depth = 10, axis=-1)\n",
        "\n",
        "            sign = tf.random_uniform([tf.shape(inputs)[0],1],minval=-1,maxval=1,dtype=tf.float32)\n",
        "            index = tf.multiply(tf.multiply(index,sign),step_size)\n",
        "            # # add zeros of unchangeable\n",
        "            split0, split1, split2, split3 = tf.split(index, [2, 1, 5, 2], axis = 1)\n",
        "            index_zeros = tf.zeros((tf.shape(inputs)[0],1))\n",
        "            index = tf.concat([split0,index_zeros,split1,index_zeros,split2,index_zeros,split3],axis=-1)\n",
        "\n",
        "            # clip of out bound valuee\n",
        "            low_bound = [1,1,2,2,6,6,6,6,10,2,2,8,8]\n",
        "            upper_bound = [10,10,25,25,25,25,25,25,100,40,40,40,40]\n",
        "\n",
        "            return tf.clip_by_value(inputs+index, clip_value_min=low_bound, clip_value_max = upper_bound)\n",
        "\n",
        "        return tf.cond(is_training, do_add, lambda: inputs, name=scope)\n",
        "\n",
        "@slim.add_arg_scope\n",
        "def step_noise_solar(inputs, step_size, is_training,  name=None):\n",
        "    with tf.name_scope(name, 'step_noise', [inputs, step_size, is_training]) as scope:\n",
        "\n",
        "        def do_add():\n",
        "            num_noise = 3\n",
        "            # add noise\n",
        "            index = tf.random_uniform([tf.shape(inputs)[0]],minval=0,maxval=num_noise,dtype=tf.int32)\n",
        "            #onehot\n",
        "            index = tf.one_hot(index,depth = num_noise, axis=-1)\n",
        "\n",
        "            sign = tf.random_uniform([tf.shape(inputs)[0],1],minval=-1,maxval=1,dtype=tf.float32)\n",
        "            index = tf.multiply(tf.multiply(index,sign),step_size)\n",
        "            # # add zeros of unchangeable\n",
        "            # split0, split1, split2, split3 = tf.split(index, [2, 1, 5, 2], axis = 1)\n",
        "            # index_zeros = tf.zeros((tf.shape(inputs)[0],1))\n",
        "            index = tf.concat([tf.zeros((tf.shape(inputs)[0],4)),index,tf.zeros((tf.shape(inputs)[0],1))],axis=-1)\n",
        "\n",
        "            # clip of out bound valuee\n",
        "            low_bound = [0]*8\n",
        "            upper_bound = [1]*8\n",
        "\n",
        "            return tf.clip_by_value(inputs+index, clip_value_min=low_bound, clip_value_max = upper_bound)\n",
        "\n",
        "        return tf.cond(is_training, do_add, lambda: inputs, name=scope)\n",
        "\n",
        "\n",
        "@slim.add_arg_scope\n",
        "def step_noise_solar24(inputs, step_size, is_training,  name=None):\n",
        "    # Features 6,7,10,19,20,23,24 are less important.\n",
        "    with tf.name_scope(name, 'step_noise', [inputs, step_size, is_training]) as scope:\n",
        "        total_num = 24\n",
        "        noise_list = [5,6,9,18,21,22,23]\n",
        "\n",
        "        def num_split(noise_list):\n",
        "            start = cur = 0\n",
        "            result=[]\n",
        "            for i in range(1, len(noise_list)):\n",
        "                if noise_list[i]==noise_list[i-1]+1:\n",
        "                    cur+=1\n",
        "                else:\n",
        "                    result.append(cur-start+1)\n",
        "                    start=cur=i\n",
        "            if cur>start:\n",
        "                result.append(cur-start+1)\n",
        "            return result\n",
        "\n",
        "        split_plan = num_split(noise_list)\n",
        "\n",
        "        def zero_concat(noise_list):\n",
        "            cur = 0\n",
        "            result=[]\n",
        "            if noise_list[0]!=0:\n",
        "                result.append(noise_list[0])\n",
        "            for i in range(1, len(noise_list)):\n",
        "                if noise_list[i]!=noise_list[i-1]+1:\n",
        "                    result.append(noise_list[i]-noise_list[i-1]-1)\n",
        "            if noise_list[-1]!=23:\n",
        "                result.append(noise_list[-1])\n",
        "            return result\n",
        "\n",
        "        zero_list = zero_concat(noise_list)\n",
        "\n",
        "\n",
        "        def do_add():\n",
        "            num_noise = len(noise_list)\n",
        "            # add noise\n",
        "            index = tf.random_uniform([tf.shape(inputs)[0]],minval=0,maxval=num_noise,dtype=tf.int32)\n",
        "            #onehot\n",
        "            index = tf.one_hot(index,depth = num_noise, axis=-1)\n",
        "\n",
        "            sign = tf.random_uniform([tf.shape(inputs)[0],1],minval=-1,maxval=1,dtype=tf.float32)\n",
        "            index = tf.multiply(tf.multiply(index,sign),step_size)\n",
        "            # # add zeros of unchangeable\n",
        "\n",
        "            split_list = tf.split(index, split_plan, axis = 1)\n",
        "            index = tf.concat([tf.zeros((tf.shape(inputs)[0],4)),index,tf.zeros((tf.shape(inputs)[0],1))],axis=-1)\n",
        "\n",
        "            merge_list = []\n",
        "            for i in range (0,4):\n",
        "                merge_list.append(tf.zeros((tf.shape(inputs)[0],zero_list[i])))\n",
        "                merge_list.append(split_list[i])\n",
        "\n",
        "            index = tf.concat(merge_list,axis=-1)\n",
        "            # clip of out bound valuee\n",
        "            low_bound = [0]*total_num\n",
        "            upper_bound = [1]*total_num\n",
        "\n",
        "            return tf.clip_by_value(inputs+index, clip_value_min=low_bound, clip_value_max = upper_bound)\n",
        "\n",
        "        return tf.cond(is_training, do_add, lambda: inputs, name=scope)\n",
        "\n",
        "\n",
        "@slim.add_arg_scope\n",
        "def gaussian_noise(inputs, scale, is_training, name=None):\n",
        "    with tf.name_scope(name, 'gaussian_noise', [inputs, scale, is_training]) as scope:\n",
        "        def do_add():\n",
        "            noise = tf.random_normal(tf.shape(inputs))\n",
        "            return inputs + noise * scale\n",
        "        return tf.cond(is_training, do_add, lambda: inputs, name=scope)\n",
        "\n",
        "@slim.add_arg_scope\n",
        "def bg_noise_layer(inputs, bg_noise_level, bg_noise_input, is_training, name=None):\n",
        "    with tf.name_scope(name, 'gaussian_noise', [bg_noise_level, is_training]) as scope:\n",
        "        def do_bg():\n",
        "            noise = tf.random_crop(bg_noise_input,size=inputs.shape[1:3])\n",
        "            noise = tf.expand_dims(noise, -1)\n",
        "            return inputs *(1-bg_noise_level) + noise * bg_noise_level\n",
        "        return tf.cond(is_training, do_bg, lambda: inputs, name=scope)\n",
        "\n",
        "@slim.add_arg_scope\n",
        "def flip_randomly(inputs, horizontally, vertically, is_training, name=None):\n",
        "    \"\"\"Flip images randomly. Make separate flipping decision for each image.\n",
        "\n",
        "    Args:\n",
        "        inputs (4-D tensor): Input images (batch size, height, width, channels).\n",
        "        horizontally (bool): If True, flip horizontally with 50% probability. Otherwise, don't.\n",
        "        vertically (bool): If True, flip vertically with 50% probability. Otherwise, don't.\n",
        "        is_training (bool): If False, no flip is performed.\n",
        "        scope: A name for the operation.\n",
        "    \"\"\"\n",
        "    with tf.name_scope(name, \"flip_randomly\") as scope:\n",
        "        batch_size, height, width, _ = tf.unstack(tf.shape(inputs))\n",
        "        vertical_choices = (tf.random_uniform([batch_size], 0, 2, tf.int32) *\n",
        "                            tf.to_int32(vertically) *\n",
        "                            tf.to_int32(is_training))\n",
        "        horizontal_choices = (tf.random_uniform([batch_size], 0, 2, tf.int32) *\n",
        "                              tf.to_int32(horizontally) *\n",
        "                              tf.to_int32(is_training))\n",
        "        vertically_flipped = tf.reverse_sequence(inputs, vertical_choices * height, 1)\n",
        "        both_flipped = tf.reverse_sequence(vertically_flipped, horizontal_choices * width, 2)\n",
        "        return tf.identity(both_flipped, name=scope)\n",
        "\n",
        "\n",
        "@slim.add_arg_scope\n",
        "def resize(inputs, scale, is_training, name=None):\n",
        "    with tf.name_scope(name, 'resize_aug', [inputs, scale, is_training]) as scope:\n",
        "        def do_resize():\n",
        "            seed = tf.random_uniform([1] ,minval=-1,maxval=1,dtype=tf.float32)\n",
        "\n",
        "            print('resize is used')\n",
        "\n",
        "            def change():\n",
        "                return tf.image.resize_bilinear(tf.image.resize_bilinear(inputs,[9,16]),[9,64])\n",
        "\n",
        "            return tf.cond(tf.less(seed,tf.convert_to_tensor([0.0]))[0],change,lambda:inputs)\n",
        "        return tf.cond(is_training, do_resize, lambda: inputs, name=scope)\n",
        "\n",
        "\n",
        "\n",
        "@slim.add_arg_scope\n",
        "def random_translate(inputs, scale, is_training,\n",
        "                     padding_mode='REFLECT', name='random_translate'):\n",
        "    \"\"\"Translate images by a random number of pixels\n",
        "    The dimensions of the image tensor remain the same. Padding is added where necessary, and the\n",
        "    pixels outside image area are cropped off.\n",
        "    For performance reasons, the offset values need to be integers and not Tensors.\n",
        "    Args:\n",
        "        inputs (4-D tensor): Input images (batch size, height, width, channels).\n",
        "        scale (integer): Maximum translation in pixels. For each image on the batch, a random\n",
        "            2-D translation is picked uniformly from ([-scale, scale], [-scale, scale]).\n",
        "        is_training (bool): If False, no translation is performed.\n",
        "        padding_mode (string): Either 'CONSTANT', 'SYMMETRIC', or 'REFLECT'. What values to use for\n",
        "            pixels that are translated from outside the original image. This parameter is passed\n",
        "            directly to tensorflow.pad fuction.\n",
        "        scope: A name for the operation.\n",
        "    \"\"\"\n",
        "    assert isinstance(scale, int)\n",
        "\n",
        "    with tf.name_scope(name) as scope:\n",
        "        def random_offsets(batch_size, minval, inclusive_maxval, name='random_offsets'):\n",
        "            with tf.name_scope(name) as scope:\n",
        "                return tf.random_uniform([batch_size],\n",
        "                                         minval=minval, maxval=inclusive_maxval + 1,\n",
        "                                         dtype=tf.int32, name=scope)\n",
        "\n",
        "        def do_translate(name='do_translate'):\n",
        "            with tf.name_scope(name) as scope:\n",
        "                batch_size = tf.shape(inputs)[0]\n",
        "                if type(scale)== int:\n",
        "                    offset_heights = random_offsets(batch_size, -scale, scale, 'offset_heights')\n",
        "                    offset_widths = random_offsets(batch_size, -scale, scale, 'offset_widths')\n",
        "                else:\n",
        "                    print('scale',scale,type(scale))\n",
        "                    offset_heights = random_offsets(batch_size, -scale[0], scale[0], 'offset_heights')\n",
        "                    offset_widths = random_offsets(batch_size, -scale[1], scale[1], 'offset_widths') \n",
        "                return translate(inputs, offset_heights, offset_widths,\n",
        "                                 scale, padding_mode, name=scope)\n",
        "\n",
        "        return tf.cond(is_training, do_translate, lambda: inputs, name=scope)\n",
        "\n",
        "\n",
        "def translate(inputs, vertical_offsets, horizontal_offsets, scale, padding_mode, name='translate'):\n",
        "    \"\"\"Translate images\n",
        "\n",
        "    The dimensions of the image remain the same. Padding is added where necessary, and the\n",
        "    pixels outside image area are cropped off.\n",
        "\n",
        "    Args:\n",
        "        inputs (4-D tensor): Input images (batch size, height, width, channels).\n",
        "        vertical_offsets (1-D tensor of integers): Vertical translation in pixels for each image.\n",
        "        horizontal offsets (1-D tensor of integers): Horizontal translation in pixels.\n",
        "        scale (integer): Maximum absolute offset (needed for performance reasons).\n",
        "        padding_mode (string): Either 'CONSTANT', 'SYMMETRIC', or 'REFLECT'. What values to use for\n",
        "            pixels that are translated from outside the original image. This parameter is passed\n",
        "            directly to tensorflow.pad fuction.\n",
        "    \"\"\"\n",
        "    assert isinstance(scale, int)\n",
        "    kernel_size = 1 + 2 * scale\n",
        "    batch_size, inp_height, inp_width, channels = inputs.get_shape().as_list()\n",
        "\n",
        "    def one_hots(offsets, name='one_hots'):\n",
        "        with tf.name_scope(name) as scope:\n",
        "            with tf.control_dependencies([tf.assert_less_equal(tf.abs(offsets), scale)]):\n",
        "                result = tf.expand_dims(tf.one_hot(scale - offsets, kernel_size), 1, name=scope)\n",
        "                assert_shape(result, [batch_size, 1, kernel_size])\n",
        "                return result\n",
        "\n",
        "    def assert_equal_first_dim(tensor_a, tensor_b, name='assert_equal_first_dim'):\n",
        "        with tf.name_scope(name) as scope:\n",
        "            first_dims = tf.shape(tensor_a)[0], tf.shape(tensor_b)[0]\n",
        "            return tf.Assert(tf.equal(*first_dims), first_dims, name=scope)\n",
        "\n",
        "    with tf.name_scope(name) as scope:\n",
        "        with tf.control_dependencies([\n",
        "            assert_equal_first_dim(inputs, vertical_offsets, \"assert_height\"),\n",
        "            assert_equal_first_dim(inputs, horizontal_offsets, \"assert_width\")\n",
        "        ]):\n",
        "            filters = tf.matmul(one_hots(vertical_offsets),\n",
        "                                one_hots(horizontal_offsets),\n",
        "                                adjoint_a=True)\n",
        "            assert_shape(filters, [batch_size, kernel_size, kernel_size])\n",
        "\n",
        "            padding_sizes = [[0, 0], [scale, scale], [scale, scale], [0, 0]]\n",
        "            padded_inp = tf.pad(inputs, padding_sizes, mode=padding_mode)\n",
        "            assert_shape(padded_inp,\n",
        "                         [batch_size, inp_height + 2 * scale, inp_width + 2 * scale, channels])\n",
        "\n",
        "            depthwise_inp = tf.transpose(padded_inp, perm=[3, 1, 2, 0])\n",
        "            assert_shape(depthwise_inp,\n",
        "                         [channels, inp_height + 2 * scale, inp_width + 2 * scale, batch_size])\n",
        "\n",
        "            depthwise_filters = tf.expand_dims(tf.transpose(filters, [1, 2, 0]), -1)\n",
        "            assert_shape(depthwise_filters, [kernel_size, kernel_size, batch_size, 1])\n",
        "\n",
        "            convoluted = tf.nn.depthwise_conv2d_native(depthwise_inp, depthwise_filters,\n",
        "                                                       strides=[1, 1, 1, 1], padding='VALID')\n",
        "            assert_shape(convoluted, [channels, inp_height, inp_width, batch_size])\n",
        "\n",
        "            result = tf.transpose(convoluted, (3, 1, 2, 0), name=scope)\n",
        "            assert_shape(result, [batch_size, inp_height, inp_width, channels])\n",
        "\n",
        "            return result\n",
        "\n",
        "\n",
        "def lrelu(inputs, leak=0.1, name=None):\n",
        "    with tf.name_scope(name, 'lrelu') as scope:\n",
        "        return tf.maximum(inputs, leak * inputs, name=scope)\n",
        "\n",
        "\n",
        "def adam_optimizer(cost, global_step,\n",
        "                   learning_rate=0.001, beta1=0.9, beta2=0.999, epsilon=1e-8,\n",
        "                   name=None):\n",
        "    with tf.name_scope(name, \"adam_optimizer\") as scope:\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate,\n",
        "                                           beta1=beta1,\n",
        "                                           beta2=beta2,\n",
        "                                           epsilon=epsilon)\n",
        "        return optimizer.minimize(cost, global_step=global_step, name=scope)\n",
        "\n",
        "def sgd_optimizer(cost, global_step,\n",
        "                   learning_rate=0.001, momentum=0.9,name=None):\n",
        "    with tf.name_scope(name, \"momentum_optimizer\") as scope:\n",
        "        optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
        "                                           momentum=momentum)\n",
        "        return optimizer.minimize(cost, global_step=global_step, name=scope)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Overwriting /content/ASTAR-HAR/mean_teacher/nn.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ewE75t9eguQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 742
        },
        "outputId": "eb7be119-1bc8-42eb-e03b-7a162bbed0e3"
      },
      "source": [
        "!python3 /content/ASTAR-HAR/experiments/har/har.py  --dataset 'HAR'"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:458: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:459: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:460: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:461: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:462: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:465: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "not divisible: 1000/24\n",
            "HAR is loaded with 18367 of training samples\n",
            "tower is initliazed!\n",
            "argmax is used for error\n",
            "argmax is used for error\n",
            "sparse softmax is used for classification loss\n",
            "sparse softmax is used for classification loss\n",
            "softmax normalization is used for consistent loss\n",
            "2020-05-13 14:41:36.051663: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.\n",
            "2020-05-13 14:41:36.051718: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.\n",
            "2020-05-13 14:41:36.051731: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.\n",
            "2020-05-13 14:41:36.051741: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.\n",
            "2020-05-13 14:41:36.051753: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX512F instructions, but these are available on your machine and could speed up CPU computations.\n",
            "2020-05-13 14:41:36.051768: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.\n",
            "Saved tensorboard graph to 'results/test_1000//2000/tensorboard/train'\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/ASTAR-HAR/experiments/har/har.py\", line 126, in <module>\n",
            "    run(**run_params)\n",
            "  File \"/content/ASTAR-HAR/experiments/har/har.py\", line 105, in run\n",
            "    model.train(training_batches, evaluation_batches_fn)\n",
            "  File \"/content/ASTAR-HAR/mean_teacher/mean_teacher_base.py\", line 198, in train\n",
            "    self.run(self.train_init_op, self.feed_dict(next(training_batches)))\n",
            "  File \"/content/ASTAR-HAR/mean_teacher/mean_teacher_base.py\", line 264, in run\n",
            "    return self.session.run(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 789, in run\n",
            "    run_metadata_ptr)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\", line 975, in _run\n",
            "    % (np_val.shape, subfeed_t.name, str(subfeed_t.get_shape())))\n",
            "ValueError: Cannot feed value of shape (100, 9, 64) for Tensor 'placeholders/images:0', which has shape '(?, 32, 32, 3)'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAYRdMlCelGo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}