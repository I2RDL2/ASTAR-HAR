{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import signal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "def preprocess_accel_(csv):\n",
    "\n",
    "    x_gyro=np.asarray(csv['x-axis (g)'])\n",
    "    y_gyro=np.asarray(csv['y-axis (g)'])\n",
    "    z_gyro=np.asarray(csv['z-axis (g)'])\n",
    "    \n",
    "    #        y=excel['y-axis(g)']\n",
    "    #        z=excel['z-axis(g)']\n",
    "    xyz_gyro=np.transpose(np.array(list(zip(x_gyro,y_gyro,z_gyro))))\n",
    "\n",
    "    ##############\n",
    "\n",
    "\n",
    "\n",
    "    def butter_lowpass(cutoff, nyq_freq, order=1):\n",
    "        normal_cutoff = float(cutoff) / nyq_freq\n",
    "        b, a = signal.butter(order, normal_cutoff, btype='lowpass')\n",
    "        #print(b,a)\n",
    "        return b, a\n",
    "\n",
    "    def butter_lowpass_filter(data, cutoff_freq, nyq_freq, order=1):\n",
    "        b, a = butter_lowpass(cutoff_freq, nyq_freq, order=order)\n",
    "        y = signal.filtfilt(b, a, data)\n",
    "        return y\n",
    "\n",
    "\n",
    "    # Filter signal x, result stored to y: \n",
    "    cutoff_frequency = 0.3\n",
    "    sample_rate=25\n",
    "    #filtered_a = butter_lowpass_filter(a, cutoff_frequency, sample_rate/2)\n",
    "\n",
    "    # Difference acts as a special high-pass from a reversed butterworth filter. \n",
    "    #diff = np.array(a)-np.array(filtered_a)\n",
    "    ###############################\n",
    "\n",
    "    x_gyro_filtered=butter_lowpass_filter(x_gyro, cutoff_frequency, sample_rate/2)\n",
    "    y_gyro_filtered=butter_lowpass_filter(y_gyro, cutoff_frequency, sample_rate/2)\n",
    "    z_gyro_filtered=butter_lowpass_filter(z_gyro, cutoff_frequency, sample_rate/2) \n",
    "\n",
    "    xyz_gyro_filtered=np.transpose(np.array(list(zip(x_gyro_filtered,y_gyro_filtered,z_gyro_filtered))))\n",
    "\n",
    "    x_gyro_diff= np.array(x_gyro)-np.array(x_gyro_filtered)\n",
    "    y_gyro_diff= np.array(y_gyro)-np.array(y_gyro_filtered)\n",
    "    z_gyro_diff= np.array(z_gyro)-np.array(z_gyro_filtered)\n",
    "\n",
    "    xyz_gyro_diff=np.transpose(np.array(list(zip(x_gyro_diff,y_gyro_diff,z_gyro_diff))))\n",
    "\n",
    "\n",
    "    \n",
    "    xyz=np.vstack((xyz_gyro_filtered,xyz_gyro_diff))\n",
    "\n",
    "\n",
    "    ###standardize\n",
    "    #xyz_scaled=np.zeros((xyz.shape[0],xyz.shape[1]))\n",
    "    xyztrans=np.transpose(xyz)\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(xyztrans)\n",
    "    #for i in range(xyz.shape[0]):\n",
    "    #    xyz_scaled[i]=scaler.transform(xyz[i])\n",
    "    xyz_scaled=np.transpose(scaler.transform(xyztrans))\n",
    "    return xyz\n",
    "def preprocess_gyro_(csv):\n",
    "\n",
    "\n",
    "    x_gyro=np.asarray(csv['x-axis (deg/s)'])\n",
    "    y_gyro=np.asarray(csv['y-axis (deg/s)'])\n",
    "    z_gyro=np.asarray(csv['z-axis (deg/s)'])\n",
    "    #        y=excel['y-axis(g)']\n",
    "    #        z=excel['z-axis(g)']\n",
    "    def butter_lowpass(cutoff, nyq_freq, order=4):\n",
    "        normal_cutoff = float(cutoff) / nyq_freq\n",
    "        b, a = signal.butter(order, normal_cutoff, btype='lowpass')\n",
    "        return b, a\n",
    "    xyz=(np.array(list(zip(x_gyro,y_gyro,z_gyro))))\n",
    "    \n",
    "    xyz_ = np.transpose(xyz)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(xyz)\n",
    "    #for i in range(xyz.shape[0]):\n",
    "    #    xyz_scaled[i]=scaler.transform(xyz[i])\n",
    "    xyz_scaled=np.transpose(np.transpose(scaler.transform(xyz)))\n",
    "    return xyz_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KazukiOsawa UnwornSensor 298 (298,) 17.0\n",
      "KazukiOsawa ClimbingUp 275 (275,) 2.0\n",
      "KazukiOsawa ClimibingDown 211 (211,) 3.0\n",
      "KazukiOsawa Jogging 285 (285,) 10.0\n",
      "KazukiOsawa Typing 376 (376,) 16.0\n",
      "KazukiOsawa Lunges 45 (45,) 11.0\n",
      "KazukiOsawa WeightLifting 34 (34,) 19.0\n",
      "KazukiOsawa Sprinting 40 (40,) 14.0\n",
      "KazukiOsawa CyclingIndoor 302 (302,) 6.0\n",
      "KazukiOsawa Crunches 44 (44,) 5.0\n",
      "KazukiOsawa Dumbbell 293 (293,) 8.0\n",
      "KazukiOsawa Walking 293 (293,) 18.0\n",
      "KazukiOsawa PushUp 24 (24,) 12.0\n",
      "KazukiOsawa CombHair 239 (239,) 4.0\n",
      "KazukiOsawa SitUp 273 (273,) 13.0\n",
      "KazukiOsawa Drinking 39 (39,) 7.0\n",
      "KazukiOsawa Squats 281 (281,) 15.0\n",
      "KazukiOsawa Brushing 323 (323,) 1.0\n",
      "KazukiOsawa Eating 284 (284,) 9.0\n",
      "KD UnwornSensor 275 (275,) 17.0\n",
      "KD ClimbingUp 272 (272,) 2.0\n",
      "KD ClimibingDown 274 (274,) 3.0\n",
      "KD Jogging 271 (271,) 10.0\n",
      "KD Typing 272 (272,) 16.0\n",
      "KD CyclingIndoor 272 (272,) 6.0\n",
      "KD Walking 271 (271,) 18.0\n",
      "KD CombHair 272 (272,) 4.0\n",
      "KD Drinking 11 (11,) 7.0\n",
      "KD Biking 272 (272,) 0.0\n",
      "KD Brushing 288 (288,) 1.0\n",
      "Arko Dutt UnwornSensor 280 (280,) 17.0\n",
      "Arko Dutt ClimbingUp 285 (285,) 2.0\n",
      "Arko Dutt ClimibingDown 274 (274,) 3.0\n",
      "Arko Dutt Jogging 273 (273,) 10.0\n",
      "Arko Dutt Typing 280 (280,) 16.0\n",
      "Arko Dutt Lunges 275 (275,) 11.0\n",
      "Arko Dutt WeightLifting 274 (274,) 19.0\n",
      "Arko Dutt Sprinting 275 (275,) 14.0\n",
      "Arko Dutt CyclingIndoor 275 (275,) 6.0\n",
      "Arko Dutt Crunches 274 (274,) 5.0\n",
      "Arko Dutt Dumbbell 284 (284,) 8.0\n",
      "Arko Dutt Walking 274 (274,) 18.0\n",
      "Arko Dutt PushUp 273 (273,) 12.0\n",
      "Arko Dutt CombHair 273 (273,) 4.0\n",
      "Arko Dutt SitUp 274 (274,) 13.0\n",
      "Arko Dutt Drinking 25 (25,) 7.0\n",
      "Arko Dutt Biking 274 (274,) 0.0\n",
      "Arko Dutt Squats 273 (273,) 15.0\n",
      "Arko Dutt Brushing 282 (282,) 1.0\n",
      "Arko Dutt Eating 281 (281,) 9.0\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "import pandas as pd\n",
    "#from leven import levenshtein       \n",
    "import numpy as np\n",
    "#import librosa\n",
    "from sklearn.cluster import dbscan\n",
    "roota = \"/home/govind/OneDrive/\" # change with one drive location\n",
    "\n",
    "test_set= ['KD','KazukiOsawa']\n",
    "valid_set= ['Arko Dutt']\n",
    "\n",
    "activities_set = ['Biking',\n",
    " 'Brushing',\n",
    " 'ClimbingUp',\n",
    " 'ClimibingDown',\n",
    " 'CombHair',\n",
    " 'Crunches',\n",
    " 'CyclingIndoor',\n",
    " 'Drinking',\n",
    " 'Dumbbell',\n",
    " 'Eating',\n",
    " \n",
    " \n",
    " 'Jogging',\n",
    " \n",
    " 'Lunges',\n",
    " 'PushUp',\n",
    " 'SitUp',\n",
    " 'Sprinting',\n",
    " 'Squats',\n",
    " 'Typing',\n",
    " 'UnwornSensor',\n",
    " 'Walking',\n",
    " 'WeightLifting']\n",
    "test_data=[]\n",
    "test_labels=[]\n",
    "train_data=[]\n",
    "train_labels =[]\n",
    "valid_data=[]\n",
    "valid_labels=[]\n",
    "window_len=64\n",
    "n_channels=9\n",
    "allowed= [\"Eating\",\"Walking\",\"UnwornSensor\",\"ClimbingDown\",\"Typing\",\"Biking\",\"Drinking\", \\\n",
    "          \"Brushing\",\"Combhair\",\"Pushup\",\"Jogging\",\"Dumbbell\"]\n",
    "allowed_val =[\"False_positive\",\"Walking\"]\n",
    "data_appended=np.zeros((n_channels,1))\n",
    "index1 =np.array([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  8.,  9., 11., 13., 14., 15.,\n",
    "       16., 17., 20., 21., 22., 23.]).astype(int).tolist()\n",
    "for root, dirs, files in os.walk(roota, topdown = False):\n",
    "    \n",
    "       subject_name= root.split(\"/\")[-1]\n",
    "       \n",
    "       for name in dirs:\n",
    "          \n",
    "          if name in activities_set:  \n",
    "            \n",
    "              #print(os.path.join(root, name),\"exre\")\n",
    "              files_num=0\n",
    "              for root1, dirs1, files1 in os.walk(os.path.join(root, name), topdown = False):\n",
    "                  \n",
    "                        \n",
    "                        for filename in files1:\n",
    "                              flag=0\n",
    "                              if filename.endswith('.csv') and 'Accelerometer' in filename:\n",
    "                                   \n",
    "                                   \n",
    "                                   data=pd.read_csv(os.path.join(root,name,filename))\n",
    "                                   files_num+=1\n",
    "                                   if len(data)>15:\n",
    "                                       preprocessed_data_accel =preprocess_accel_(data) \n",
    "                                       \n",
    "                                   prefix=filename.split('Accelerometer')[0]\n",
    "                                   for cc in sorted(os.listdir(os.path.join(root,name))):\n",
    "                                     if prefix in cc and 'Gyroscope' in cc:\n",
    "                                        flag=1\n",
    "                                        \n",
    "                                        filename=cc\n",
    "                                \n",
    "                                        data=pd.read_csv(os.path.join(root,name,filename))\n",
    "\n",
    "                                        if len(data)>15:\n",
    "                                           preprocessed_data_gyro =preprocess_gyro_(data) \n",
    "                                           \n",
    "                                           flag=1\n",
    "                                   \n",
    "                              \n",
    "                              if flag:\n",
    "                                if preprocessed_data_accel.shape[1]<=preprocessed_data_gyro.shape[1] :\n",
    "                                    data_temp= np.concatenate((preprocessed_data_gyro[:,:preprocessed_data_accel.shape[1]],preprocessed_data_accel),axis=0)#,dat_ambient[np.newaxis,:preprocessed_data_accel.shape[1]],dat_pressure[np.newaxis,:preprocessed_data_accel.shape[1]]),axis=0)\n",
    "                                    #y=np.reshape(data_temp,-1,order='F')\n",
    "\n",
    "                                    data_appended= np.concatenate((data_appended,data_temp),axis=1)\n",
    "                                    #print(data_temp.shape)\n",
    "                                    y=np.reshape(data_temp,-1,order='F')\n",
    "                                    ### assume series length is window_len samples and n_channels channels for inital use\n",
    "                                    z= y[:int(np.floor(y.shape[0]//(window_len*n_channels))*window_len*n_channels)]\n",
    "                                    z=z.reshape(-1,window_len,n_channels) \n",
    "                                    data_processed1=np.transpose(z,(0,2,1))\n",
    "                                    y=y[(n_channels*(window_len//2)):] # change here for overlap percentage /2 is 50% /4 is 25%\n",
    "                                    z= y[:int(np.floor(y.shape[0]//(window_len*n_channels))*window_len*n_channels)]\n",
    "                                    z=z.reshape(-1,window_len,n_channels) \n",
    "                                    data_processed2=np.transpose(z,(0,2,1))\n",
    "                                    #print( (os.path.join(root1,name2)))\n",
    "                                    \n",
    "                                    #label= sorted(dict_exp_people.keys()).index(name2)\n",
    "                                    data_=np.concatenate((data_processed1,data_processed2),axis=0)\n",
    "                                    #print(data_.shape)\n",
    "                                    scaler = StandardScaler()\n",
    "                                    labels= np.zeros(data_.shape[0])\n",
    "#                                     if name==\"Walking\":\n",
    "#                                                     labels[:]=2\n",
    "#                                                     print(subject_name)\n",
    "                                    #perm_lis=list( np.random.permutation(data_.shape[0]))\n",
    "                                    \n",
    "                                    #print(data_.shape,labels.shape)\n",
    "                                    \n",
    "                                    labels[:]=activities_set.index(name)\n",
    "                                    if subject_name in test_set:\n",
    "                                         \n",
    "                                            \n",
    "                                            print(subject_name, name,data_.shape[0],labels.shape,labels.mean())\n",
    "\n",
    "\n",
    "                                            test_data.append(data_)\n",
    "                                            test_labels.append(labels)\n",
    "                                    elif subject_name in valid_set:\n",
    "                                         \n",
    "                                            \n",
    "                                            print(subject_name, name,data_.shape[0],labels.shape,labels.mean())\n",
    "\n",
    "\n",
    "                                            valid_data.append(data_)\n",
    "                                            valid_labels.append(labels)\n",
    "                                    else :\n",
    "                                        train_data.append(data_)\n",
    "                                        train_labels.append(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59692.0 (6709, 9, 64)\n",
      "(21387,) (21387, 9, 64)\n",
      "Walking          3029\n",
      "Eating           2495\n",
      "Typing           2287\n",
      "UnwornSensor     1378\n",
      "Jogging          1217\n",
      "Biking           1167\n",
      "CyclingIndoor    1019\n",
      "ClimbingUp        974\n",
      "CombHair          964\n",
      "Lunges            921\n",
      "WeightLifting     893\n",
      "SitUp             805\n",
      "Crunches          722\n",
      "Brushing          650\n",
      "Squats            637\n",
      "Sprinting         618\n",
      "ClimibingDown     582\n",
      "PushUp            435\n",
      "Dumbbell          382\n",
      "Drinking          212\n",
      "Name: labels, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#print(test_labels,subject_name)\n",
    "import numpy as np\n",
    "import scipy\n",
    "print(np.concatenate(test_labels,axis=0).sum(),np.concatenate(test_data,axis=0).shape)\n",
    "print(np.concatenate(train_labels,axis=0).shape,np.concatenate(train_data,axis=0).shape)\n",
    "train_labels1= np.concatenate(train_labels,axis=0)\n",
    "val_labels1= np.concatenate(valid_labels,axis=0)\n",
    "test_labels1= np.concatenate(test_labels,axis=0)\n",
    "#print(np.sum(train_labels1),(test_labels))\n",
    "\n",
    "import pandas as pd\n",
    "dft=pd.DataFrame(train_labels1.tolist(),columns=[\"labels\"])\n",
    "\n",
    "for i in range(20):\n",
    "    dft['labels']=dft['labels'].replace(float(i),activities_set[i]) \n",
    "print(dft['labels'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "np.histogram(train_labels1,bins=np.arange(24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shitala subject\n",
      "21\n",
      "darayus_1 subject\n",
      "34\n",
      "Darayus Patel subject\n",
      "51\n",
      "gaurav subject\n",
      "0 fallgau1_MetaWear_2019-11-11T17.38.33.478_EAF014A034EC_Gyroscope_25.000Hz_1.3.6.csv\n",
      "1 fallgau2_MetaWear_2019-11-11T17.39.21.531_EAF014A034EC_Gyroscope_25.000Hz_1.3.6.csv\n",
      "2 fallgau5_MetaWear_2019-11-11T17.41.46.023_EAF014A034EC_Gyroscope_25.000Hz_1.3.6.csv\n",
      "3 fallgau7_MetaWear_2019-11-11T17.44.39.580_EAF014A034EC_Gyroscope_50.000Hz_1.3.6.csv\n",
      "55\n",
      "suprajit_1 subject\n",
      "66\n",
      "Ahmed Zaky subject\n",
      "85\n",
      "KazukiOsawa subject\n",
      "104\n",
      "govind_1 subject\n",
      "117\n",
      "KD subject\n",
      "4 KDfor1_Ahmed_2019-11-11T17.41.49.282_F447460F942B_Gyroscope_25.000Hz_1.3.6.csv\n",
      "5 KDfor2_Ahmed_2019-11-11T17.42.24.470_F447460F942B_Gyroscope_25.000Hz_1.3.6.csv\n",
      "6 KDfor3_Ahmed_2019-11-11T17.43.24.519_F447460F942B_Gyroscope_25.000Hz_1.3.6.csv\n",
      "7 KDfor4_Ahmed_2019-11-11T17.44.10.334_F447460F942B_Gyroscope_25.000Hz_1.3.6.csv\n",
      "8 KDfor5_Ahmed_2019-11-11T17.44.45.778_F447460F942B_Gyroscope_25.000Hz_1.3.6.csv\n",
      "9 KDinno1_Ahmed_2019-11-11T17.45.39.523_F447460F942B_Gyroscope_25.000Hz_1.3.6.csv\n",
      "10 KDinno2_Ahmed_2019-11-11T17.46.27.427_F447460F942B_Gyroscope_25.000Hz_1.3.6.csv\n",
      "11 KDinno3_Ahmed_2019-11-11T17.47.47.425_F447460F942B_Gyroscope_25.000Hz_1.3.6.csv\n",
      "12 KDinno4_Ahmed_2019-11-11T17.48.26.005_F447460F942B_Gyroscope_25.000Hz_1.3.6.csv\n",
      "13 KDinno5_Ahmed_2019-11-11T17.49.08.008_F447460F942B_Gyroscope_25.000Hz_1.3.6.csv\n",
      "14 KDleft1_Ahmed_2019-11-11T17.33.48.887_F447460F942B_Gyroscope_25.000Hz_1.3.6.csv\n",
      "15 KDleft2_Ahmed_2019-11-11T17.35.35.752_F447460F942B_Gyroscope_25.000Hz_1.3.6.csv\n",
      "16 KDleft3_Ahmed_2019-11-11T17.36.08.030_F447460F942B_Gyroscope_25.000Hz_1.3.6.csv\n",
      "17 KDleft4_Ahmed_2019-11-11T17.37.58.791_F447460F942B_Gyroscope_25.000Hz_1.3.6.csv\n",
      "18 KDleft5_Ahmed_2019-11-11T17.37.13.152_F447460F942B_Gyroscope_25.000Hz_1.3.6.csv\n",
      "19 KDright1_Ahmed_2019-11-11T17.38.32.133_F447460F942B_Gyroscope_25.000Hz_1.3.6.csv\n",
      "20 KDright2_Ahmed_2019-11-11T17.39.11.078_F447460F942B_Gyroscope_25.000Hz_1.3.6.csv\n",
      "21 KDright3_Ahmed_2019-11-11T17.39.45.608_F447460F942B_Gyroscope_25.000Hz_1.3.6.csv\n",
      "22 KDright4_Ahmed_2019-11-11T17.41.04.887_F447460F942B_Gyroscope_25.000Hz_1.3.6.csv\n",
      "23 KDright5_Ahmed_2019-11-11T17.40.23.502_F447460F942B_Gyroscope_25.000Hz_1.3.6.csv\n",
      "137\n",
      "test_fall subject\n",
      "152\n",
      "Arko Dutt subject\n",
      "170\n",
      "DEMO subject\n",
      "175\n",
      "arko_1 subject\n",
      "183\n"
     ]
    }
   ],
   "source": [
    "import sys,os\n",
    "import pandas as pd\n",
    "    \n",
    "import numpy as np\n",
    "#import librosa\n",
    "\n",
    "from sklearn.cluster import dbscan\n",
    "\n",
    "#roota = \"/DL2-Target1/govind/OneDrive/\"\n",
    "roota = \"/home/govind/OneDrive/\"\n",
    "\n",
    "test_set=['KD',\"gaurav\"]\n",
    "valid_set =['suprajit_1']\n",
    "window_len=64\n",
    "n_channels=9\n",
    "#data_appended=np.zeros((n_channels,1))\n",
    "dict_people_exp= {}\n",
    "dict_exp_time={}\n",
    "dict_exp_people={}\n",
    "flag2=0\n",
    "count=0\n",
    "llb=0\n",
    "for root, dirs, files in os.walk(roota, topdown = False):\n",
    "#    for name in files:\n",
    "#       print(os.path.join(root, name))\n",
    "       #print(root,dirs)\n",
    "       if dirs:\n",
    "            \n",
    "           #print(dirs,'dirs',root)\n",
    "           subject_name= root.split(\"/\")[-1]\n",
    "           for name in dirs:\n",
    "             # if 'govind' in name:\n",
    "                  #print(os.path.join(root, name),'top')\n",
    "                #print(name,'exercise')\n",
    "                              \n",
    "                              if name=='Falling':\n",
    "                                    print(subject_name,\"subject\")\n",
    "\n",
    "\n",
    "\n",
    "                                    flag=0\n",
    "\n",
    "                                    for filename in sorted(os.listdir(os.path.join(root,name))):\n",
    "\n",
    "                                       flag=0\n",
    "                                       if filename.endswith('.csv') and 'Accelerometer' in filename:\n",
    "\n",
    "                                           flag=1\n",
    "                                           data=pd.read_csv(os.path.join(root,name,filename))\n",
    "\n",
    "                                       #print(data.columns)\n",
    "                                       if len(data)>15:\n",
    "                                           preprocessed_data_accel =preprocess_accel_(data) \n",
    "\n",
    "                                           prefix=filename.split('Accelerometer')[0]\n",
    "                                           for cc in sorted(os.listdir(os.path.join(root,name))):\n",
    "                                             if prefix in cc and 'Gyroscope' in cc:\n",
    "\n",
    "                                                #print(prefix)\n",
    "                                                filename=cc\n",
    "                                                #print(filename)\n",
    "                                                data1=pd.read_csv(os.path.join(root,name,filename))\n",
    "                                                #print(data.columns)\n",
    "                                                #print(data1.keys(),flag)\n",
    "                                                if len(data1)>15:\n",
    "                                                   files_num+=1\n",
    "                                                   preprocessed_data_gyro =preprocess_gyro_(data1) \n",
    "                                                   flag=flag+1\n",
    "                                       #print(flag,'flag',files_num)\n",
    "                                       if flag==2:\n",
    "                                        #print(preprocessed_data_accel.shape[1],preprocessed_data_gyro.shape[1])    \n",
    "                                        if preprocessed_data_accel.shape[1]<=preprocessed_data_gyro.shape[1] :\n",
    "                                            count+=1\n",
    "                                            data_temp= np.concatenate((preprocessed_data_gyro[:,:preprocessed_data_accel.shape[1]],preprocessed_data_accel),axis=0)#,dat_ambient[np.newaxis,:preprocessed_data_accel.shape[1]],dat_pressure[np.newaxis,:preprocessed_data_accel.shape[1]]),axis=0)\n",
    "                                            #y=np.reshape(data_temp,-1,order='F')\n",
    "\n",
    "                                            data_appended= np.concatenate((data_appended,data_temp),axis=1)\n",
    "                                            #print(data_appended.shape,data_temp.shape)\n",
    "\n",
    "                                            if data_temp.shape[1]<window_len:\n",
    "                                                #print(data_temp.shape)\n",
    "                                                data_temp= np.concatenate((np.tile(data_temp[:,0]\n",
    "                                                                                   [:,np.newaxis],(1,window_len-data_temp.shape[1])),data_temp)\n",
    "                                                                          ,axis=1)\n",
    "\n",
    "                                            y=np.reshape(data_temp,-1,order='F')\n",
    "\n",
    "                                            ### assume series length is window_len samples and n_channels channels for inital use\n",
    "                                            temp_num=int(np.floor(y.shape[0]//(window_len*n_channels))*window_len*n_channels)\n",
    "                                            #print(temp_num,\"inde\")\n",
    "                                            z= y[y.shape[0]-int(np.floor(y.shape[0]//(window_len*n_channels))*window_len*n_channels):]\n",
    "                                            z=z.reshape(-1,window_len,n_channels) \n",
    "                                            data_processed1=np.transpose(z,(0,2,1))\n",
    "                                            #print(data_processed1.shape)\n",
    "    #                                         y=y[(n_channels*(window_len//2)):] # change here for overlap percentage /2 is 50% /4 is 25%\n",
    "    #                                         z= y[:int(np.floor(y.shape[0]//(window_len*n_channels))*window_len*n_channels)]\n",
    "    #                                         z=z.reshape(-1,window_len,n_channels) \n",
    "    #                                         data_processed2=np.transpose(z,(0,2,1))\n",
    "                                            #print( (os.path.join(root1,name2)))\n",
    "\n",
    "                                            #label= sorted(dict_exp_people.keys()).index(name2)\n",
    "                                            #data_=np.concatenate((data_processed1,data_processed2),axis=0)\n",
    "                                            #scaler = StandardScaler()\n",
    "                                            #print(data_processed1.shape[0],\"no:of data_snippets\" )\n",
    "                                            labels= np.zeros(1)\n",
    "                                            labels[:]=1 #activities_set.index(\"Falling\")\n",
    "\n",
    "                                            if subject_name in test_set:                                    \n",
    "                                                test_data.append(data_processed1[-1][np.newaxis,:,:])\n",
    "                                                test_labels.append(labels)\n",
    "                                                print(llb,filename)\n",
    "                                                llb+=1\n",
    "                                            elif subject_name in valid_set:\n",
    "                                                valid_data.append(data_processed1[-1][np.newaxis,:,:])\n",
    "                                                valid_labels.append(labels)\n",
    "                                            else :\n",
    "                                                train_data.append(data_processed1[-1][np.newaxis,:,:])\n",
    "                                                train_labels.append(labels)\n",
    "                                    #print(files_num,\"files\")        \n",
    "                                    print(count)        \n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 64)\n",
      "(171, 6) (117, 6) darayus_twist3_MetaWear1_2020-01-31T20.11.54.459_EFFC0CE06F0C_Accelerometer_25.000Hz.csv darayus_twist3_MetaWear1_2020-01-31T20.11.54.459_EFFC0CE06F0C_\n"
     ]
    }
   ],
   "source": [
    "print(data_processed1[-1].shape)\n",
    "print(data1.shape,data.shape,cc,prefix)\n",
    "x_gyro=np.asarray(data['x-axis (g)'])\n",
    "y_gyro=np.asarray(data['y-axis (g)'])\n",
    "z_gyro=np.asarray(data['z-axis (g)'])\n",
    "\n",
    "#        y=excel['y-axis(g)']\n",
    "#        z=excel['z-axis(g)'] \n",
    "xyz_gyro=np.vstack((x_gyro,y_gyro,z_gyro))\n",
    "np.savetxt('data_raw_accel.txt',xyz_gyro)  \n",
    "x_gyro=np.asarray(data1['x-axis (deg/s)'])\n",
    "y_gyro=np.asarray(data1['y-axis (deg/s)'])\n",
    "z_gyro=np.asarray(data1['z-axis (deg/s)'])\n",
    "xyz_gyro=np.vstack((x_gyro,y_gyro,z_gyro))\n",
    "np.savetxt('data_raw_gyro.txt',xyz_gyro)  \n",
    "np.savetxt(\"preprocessed_gyro.txt\",preprocessed_data_gyro)\n",
    "np.savetxt(\"preprocessed_accel.txt\",preprocessed_data_accel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([], dtype=float64),\n",
       " array([], dtype=float64),\n",
       " array([0.]),\n",
       " array([], dtype=float64),\n",
       " array([], dtype=float64),\n",
       " array([], dtype=float64),\n",
       " array([], dtype=float64),\n",
       " array([], dtype=float64),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0.]),\n",
       " array([], dtype=float64),\n",
       " array([0.]),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.]),\n",
       " array([1.])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import mode\n",
    "import tensorflow as tf\n",
    "test_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[71.43524911 60.61765604 75.07813242  0.65226739  0.73344189  0.65460851] [-0.75013719 -0.0438054  -0.33871264 -0.09064646 -0.06198356  0.33541542]\n",
      "indata_z: 128.000000\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "data_out=scaler.transform(np.transpose(data_processed1[-1]))\n",
    "np.savetxt('data_after_standardization_falling.txt',data_out)\n",
    "#print(np.max(data_out))\n",
    "hist, bin_edges = np.histogram(data_out.reshape(-1))\n",
    "#print(hist)\n",
    "plt.hist(hist,bin_edges-1)\n",
    "print(np.sqrt(scaler.var_),scaler.mean_)\n",
    "input_max=2\n",
    "input_min=-2\n",
    "bits = 8\n",
    "bits_n = 2**bits - 1\n",
    "Si = (input_max-input_min) / bits_n\n",
    "indata_q = np.round((data_out- input_min) / Si)\n",
    "indata_z = np.round(-input_min / Si)\n",
    "print('indata_z: %f\\n' % indata_z)\n",
    "indata_int = indata_q - indata_z\n",
    "indata_int = np.clip(indata_int.astype(np.int32),-127,128)\n",
    "np.savetxt('data_after_standardization_and_quantizatioin_falling.txt',indata_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "~/GAN-manifold-regularization-master/HAR_ICASP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0, 13.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 21.0] unique [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 7. 7. 7. 7.\n",
      " 7. 7. 7. 7. 7. 7. 7.]\n",
      "(21814, 9, 64) (6945, 9, 64)\n",
      "[ 0.  1.  2.  3.  4.  5.  6.  8.  9. 11. 13. 15. 16. 17. 18. 19. 20. 21.] [3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.\n",
      " 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import mode\n",
    "import tensorflow as tf\n",
    "#tf.enable_eager_execution()\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(np.transpose(data_appended))\n",
    "def scaled(x):\n",
    "    for i in range(x.shape[0]):\n",
    "        x[i]=np.transpose(scaler.transform(np.transpose(x[i])))\n",
    "    return x\n",
    "\n",
    "test_arr= np.concatenate(test_data,axis=0).astype(np.float32)\n",
    "test_labels1= np.concatenate(test_labels)\n",
    "test_len= test_arr.shape[0]\n",
    "\n",
    "unique=list(np.unique(test_labels1))\n",
    "print(unique,\"unique\",test_labels1[-31:])\n",
    "ind_random= np.random.permutation(test_len)\n",
    "# test_arr=test_arr[ind_random]\n",
    "# test_labels1=test_labels1[ind_random]\n",
    "val_arr= np.concatenate(valid_data,axis=0).astype(np.float32)\n",
    "val_labels1= np.concatenate(valid_labels)\n",
    "val_len= val_arr.shape[0]\n",
    "\n",
    "train_arr=np.concatenate(train_data,axis=0).astype(np.float32)\n",
    "\n",
    "        \n",
    "train_labels1=np.concatenate(train_labels)\n",
    "ind=np.array([])\n",
    "ind1 = np.array([])\n",
    "\n",
    "ind=ind.astype(int)\n",
    "ind1=ind1.astype(int)\n",
    "# # a,b=np.histogram(test_labels1,bins=range(22))\n",
    "# # c,d =np.histogram(train_labels1,bins=range(22))\n",
    "# # print(a,'\\n',b)\n",
    "# # print(c,d)\n",
    "ind_random= np.random.permutation(train_arr.shape[0])\n",
    "train_arr= train_arr[ind_random]\n",
    "train_labels1=train_labels1[ind_random]\n",
    "\n",
    "# test_labels1= test_labels1[ind1]\n",
    "# test_arr= test_arr[ind1]\n",
    "\n",
    "#print(np.unique(train_labels1))\n",
    "frame_length= 16\n",
    "frame_step=4\n",
    "train_arr =scaled(train_arr)\n",
    "test_arr= scaled(test_arr)\n",
    "val_arr= scaled(val_arr)\n",
    "\n",
    "# fft_train =tf.signal.stft(    train_arr,    frame_length,    frame_step,    fft_length=None,    window_fn=tf.signal.hann_window,\n",
    "#     pad_end=True,    name='stft'\n",
    "# )\n",
    "# fft_test=tf.signal.stft(\n",
    "#     test_arr,\n",
    "#     frame_length,\n",
    "#     frame_step,\n",
    "#     fft_length=None,\n",
    "#     window_fn=tf.signal.hann_window,\n",
    "#     pad_end=True,\n",
    "#     name='stft2'\n",
    "# )\n",
    "\n",
    "print(np.unique(train_labels1),(test_labels1[-39:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n",
       "       2., 2., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_labels1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "savename = './HAR_falling_9channels_22activities'\n",
    "np.savez(savename,X_test=test_arr,Y_test=test_labels1,X_train=train_arr,\\\n",
    "         Y_train=train_labels1,X_val=val_arr,Y_val=val_labels1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def balanced_batch_generator(x, y, batch_size, categorical=True):\n",
    "    \"\"\"A generator for creating balanced batched.\n",
    "    This generator loops over its data indefinitely and yields balanced,\n",
    "    shuffled batches.\n",
    "    Args:\n",
    "    x (numpy.ndarray): Samples (inputs). Must have the same length as `y`.\n",
    "    y (numpy.ndarray): Labels (targets). Must be a binary class matrix (i.e.,\n",
    "        shape `(num_samples, num_classes)`). You can use `keras.utils.to_categorical`\n",
    "        to convert a class vector to a binary class matrix.\n",
    "    batch_size (int): Batch size.\n",
    "    categorical (bool, optional): If true, generates binary class matrices\n",
    "        (i.e., shape `(num_samples, num_classes)`) for batch labels (targets).\n",
    "        Otherwise, generates class vectors (i.e., shape `(num_samples, )`).\n",
    "        Defaults to `True`.\n",
    "    Returns a generator yielding batches as tuples `(inputs, targets)` that can\n",
    "        be directly used with Keras.\n",
    "    \"\"\"\n",
    "    if x.shape[0] != y.shape[0]:\n",
    "        raise ValueError('Args `x` and `y` must have the same length.')\n",
    "    if len(y.shape) != 2:\n",
    "        raise ValueError(\n",
    "            'Arg `y` must have a shape of (num_samples, num_classes). ' +\n",
    "            'You can use `keras.utils.to_categorical` to convert a class vector ' +\n",
    "            'to a binary class matrix.'\n",
    "        )\n",
    "    if batch_size < 1:\n",
    "        raise ValueError('Arg `batch_size` must be a positive integer.')\n",
    "    num_samples = y.shape[0]\n",
    "    num_classes = y.shape[1]\n",
    "    batch_x_shape = (batch_size, *x.shape[1:])\n",
    "    batch_y_shape = (batch_size, num_classes) if categorical else (batch_size, )\n",
    "    indexes = [0 for _ in range(num_classes)]\n",
    "    print(indexes)\n",
    "    samples = [[] for _ in range(num_classes)]\n",
    "    for i in range(num_samples):\n",
    "        samples[np.argmax(y[i])].append(x[i])\n",
    "    while True:\n",
    "        batch_x = np.ndarray(shape=batch_x_shape, dtype=x.dtype)\n",
    "        batch_y = np.zeros(shape=batch_y_shape, dtype=y.dtype)\n",
    "        for i in range(batch_size):\n",
    "            random_class = random.randrange(num_classes)\n",
    "            current_index = indexes[random_class]\n",
    "            #print(random_class)\n",
    "            indexes[random_class] = (current_index + 1) % len(samples[random_class])\n",
    "            if current_index == 0:\n",
    "                random.shuffle(samples[random_class])\n",
    "            batch_x[i] = samples[random_class][current_index]\n",
    "            if categorical:\n",
    "                batch_y[i][random_class] = 1\n",
    "            else:\n",
    "                batch_y[i] = random_class\n",
    "        #batch_y=batch_y[:,1]\n",
    "        yield (batch_x, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 1.] [0. 1.]\n",
      "121.0 (122, 2)\n",
      "11893.0 (12044, 2)\n",
      "[0, 0]\n",
      "0.5 (64, 2)\n",
      "(122, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "dat=np.load(savename)\n",
    "train_arr=dat[\"X_train\"]\n",
    "test_arr= dat[\"X_test\"]\n",
    "test_labels1= dat[\"Y_test\"]\n",
    "train_labels1 =dat[\"Y_train\"]\n",
    "print(np.unique(train_labels1),np.unique(test_labels1))\n",
    "#X_train, X_val, y_train, y_val = train_test_split(train_arr[:,:,:,np.newaxis],tf.keras.utils.to_categorical(train_labels1), test_size=0.01, random_state=42)\n",
    "print(np.sum(y_val[:,0]),y_val.shape)\n",
    "print(np.sum(y_train[:,0]),y_train.shape)\n",
    "batch_size=64\n",
    "#y_val= y_val[:,1]\n",
    "batch_generator = balanced_batch_generator(X_train,y_train, batch_size, categorical=True)\n",
    "val_generator = balanced_batch_generator(X_val,y_val, batch_size, categorical=True)\n",
    "for xx,yy in batch_generator:\n",
    "    print(np.mean(yy),yy.shape)\n",
    "    break\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "savename = './HAR_falling_all_activities_19.npz'\n",
    "np.savez(savename,X_test=test_arr,Y_test=test_labels1,X_train=train_arr,Y_train=train_labels1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2_1",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
